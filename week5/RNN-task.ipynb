{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating names with recurrent neural networks\n",
    "\n",
    "This time you'll find yourself delving into the heart (and other intestines) of recurrent neural networks on a class of toy problems.\n",
    "\n",
    "Struggle to find a name for the variable? Let's see how you'll come up with a name for your son/daughter. Surely no human has expertize over what is a good child name, so let us train RNN instead;\n",
    "\n",
    "It's dangerous to go alone, take these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our data\n",
    "The dataset contains ~8k earthling names from different cultures, all in latin transcript.\n",
    "\n",
    "This notebook has been designed so as to allow you to quickly swap names for something similar: deep learning article titles, IKEA furniture, pokemon names, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "start_token = \" \"\n",
    "\n",
    "with open(\"names\") as f:\n",
    "    names = f.read()[:-1].split('\\n')\n",
    "    names = [start_token+name for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n samples =  7944\n",
      " Abagael\n",
      " Claresta\n",
      " Glory\n",
      " Liliane\n",
      " Prissie\n",
      " Geeta\n",
      " Giovanne\n",
      " Piggy\n"
     ]
    }
   ],
   "source": [
    "print ('n samples = ',len(names))\n",
    "for x in names[::1000]:\n",
    "    print (x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length = 16\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c215f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_LENGTH = max(map(len,names))\n",
    "print(\"max length =\", MAX_LENGTH)\n",
    "\n",
    "plt.title('Sequence length distribution')\n",
    "plt.hist(list(map(len,names)),bins=25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing\n",
    "\n",
    "First we need next to collect a \"vocabulary\" of all unique tokens i.e. unique characters. We can then encode inputs as a sequence of character ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_tokens =  55\n"
     ]
    }
   ],
   "source": [
    "#all unique characters go here\n",
    "tokens = set()\n",
    "for name in names:\n",
    "    tokens = tokens.union(set(name))\n",
    "\n",
    "tokens = list(tokens)\n",
    "\n",
    "n_tokens = len(tokens)\n",
    "print ('n_tokens = ',n_tokens)\n",
    "\n",
    "assert 50 < n_tokens < 60\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cast everything from symbols into identifiers\n",
    "\n",
    "Tensorflow string manipulation is a bit tricky, so we'll work around it. \n",
    "We'll feed our recurrent neural network with ids of characters from our dictionary.\n",
    "\n",
    "To create such dictionary, let's assign "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_to_id = {tokens[i]:i for i in range(len(tokens))}###YOUR CODE HERE: create a dictionary of {symbol -> its  index in tokens }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems alright!\n"
     ]
    }
   ],
   "source": [
    "assert len(tokens) == len(token_to_id), \"dictionaries must have same size\"\n",
    "\n",
    "for i in range(n_tokens):\n",
    "    assert token_to_id[tokens[i]] == i, \"token identifier must be it's position in tokens list\"\n",
    "\n",
    "print(\"Seems alright!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_matrix(names,max_len=None,pad=0,dtype='int32'):\n",
    "    \"\"\"Casts a list of names into rnn-digestable matrix\"\"\"\n",
    "    \n",
    "    max_len = max_len or max(map(len,names))\n",
    "    names_ix = np.zeros([len(names),max_len],dtype) + pad\n",
    "\n",
    "    for i in range(len(names)):\n",
    "        name_ix = list(map(token_to_id.get,names[i]))\n",
    "        names_ix[i,:len(name_ix)] = name_ix\n",
    "\n",
    "    return names_ix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Abagael\n",
      " Glory\n",
      " Prissie\n",
      " Giovanne\n",
      "[[ 1 25 11 37 34 37 40 52  0]\n",
      " [ 1  7 52 12 47  0  0  0  0]\n",
      " [ 1  3 47 36 43 43 36 40  0]\n",
      " [ 1  7 36 12  5 37  9  9 40]]\n"
     ]
    }
   ],
   "source": [
    "#Example: cast 4 random names to matrices, pad with zeros\n",
    "print('\\n'.join(names[::2000]))\n",
    "print(to_matrix(names[::2000]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural network\n",
    "\n",
    "We can rewrite recurrent neural network as a consecutive application of dense layer to input $x_t$ and previous rnn state $h_t$. This is exactly what we're gonna do now.\n",
    "<img src=\"./rnn.png\" width=480>\n",
    "\n",
    "Since we're training a language model, there should also be:\n",
    "* An embedding layer that converts character id x_t to a vector.\n",
    "* An output layer that predicts probabilities of next phoneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Concatenate,Dense,Embedding\n",
    "\n",
    "rnn_num_units = 64\n",
    "embedding_size = 16\n",
    "\n",
    "#Let's create layers for our recurrent network\n",
    "#Note: we create layers but we don't \"apply\" them yet\n",
    "embed_x = Embedding(n_tokens, embedding_size) # an embedding layer that converts character ids into embeddings\n",
    "\n",
    "\n",
    "#a dense layer that maps input and previous state to new hidden state, [x_t,h_t]->h_t+1\n",
    "Concatenate()([])\n",
    "get_h_next = Dense(embedding_size, activation='tanh') ###YOUR CODE HERE\n",
    "\n",
    "#a dense layer that maps current hidden state to probabilities of characters [h_t+1]->P(x_t+1|h_t+1)\n",
    "get_probas = Dense() ###YOUR CODE HERE \n",
    "\n",
    "#Note: please either set the correct activation to Dense or write it manually in rnn_one_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_one_step(x_t, h_t):\n",
    "    \"\"\"\n",
    "    Recurrent neural network step that produces next state and output\n",
    "    given prev input and previous state.\n",
    "    We'll call this method repeatedly to produce the whole sequence.\n",
    "    \n",
    "    Follow inline isntructions to complete the function.\n",
    "    \"\"\"\n",
    "    #convert character id into embedding\n",
    "    x_t_emb = embed_x(tf.reshape(x_t,[-1,1]))[:,0]\n",
    "    \n",
    "    #concatenate x embedding and previous h state\n",
    "    x_and_h = ###YOUR CODE HERE\n",
    "    \n",
    "    #compute next state given x_and_h\n",
    "    h_next = ###YOUR CODE HERE\n",
    "    \n",
    "    #get probabilities for language model P(x_next|h_next)\n",
    "    output_probas = ###YOUR CODE HERE\n",
    "    \n",
    "    return output_probas,h_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN loop\n",
    "\n",
    "Once rnn_one_step is ready, let's apply it in a loop over name characters to get predictions.\n",
    "\n",
    "Let's assume that all names are at most length-16 for now, so we can simply iterate over them in a for loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sequence = tf.placeholder('int32',(MAX_LENGTH,None))\n",
    "batch_size = tf.shape(input_sequence)[1]\n",
    "\n",
    "predicted_probas = []\n",
    "h_prev = tf.zeros([batch_size,rnn_num_units]) #initial hidden state\n",
    "\n",
    "for t in range(MAX_LENGTH):\n",
    "    x_t = input_sequence[t]\n",
    "    probas_next,h_next = rnn_one_step(x_t,h_prev)\n",
    "    \n",
    "    h_prev = h_next\n",
    "    predicted_probas.append(probas_next)\n",
    "    \n",
    "predicted_probas = tf.stack(predicted_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN: loss and gradients\n",
    "\n",
    "Let's gather a matrix of predictions for $P(x_{next}|h)$ and the corresponding correct answers.\n",
    "\n",
    "Our network can then be trained by minimizing crossentropy between predicted probabilities and those answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_matrix = tf.reshape(predicted_probas[:-1],[-1,len(tokens)])\n",
    "answers_matrix = tf.one_hot(tf.reshape(input_sequence[1:],[-1]), n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = <define loss as categorical crossentropy. Mind that predictions are probabilities and NOT logits!>\n",
    "\n",
    "optimize = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from random import sample\n",
    "s = keras.backend.get_session()\n",
    "s.run(tf.global_variables_initializer())\n",
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(1000):\n",
    "    batch = to_matrix(sample(names,32),max_len=MAX_LENGTH)\n",
    "    loss_i,_ = s.run([loss,optimize],{input_sequence:batch})\n",
    "    \n",
    "    \n",
    "    history.append(loss_i)\n",
    "    if (i+1)%100==0:\n",
    "        clear_output(True)\n",
    "        plt.plot(history,label='loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN: sampling\n",
    "Once we've trained our network a bit, let's get to actually generating stuff. All we need is the `rnn_one_step` function you have written above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_t = tf.placeholder('int32',(None,))\n",
    "h_t = tf.Variable(np.zeros([1,rnn_num_units],'float32'))\n",
    "\n",
    "next_probs,next_h = rnn_one_step(x_t,h_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sample(seed_phrase=' ',max_length=MAX_LENGTH):\n",
    "    '''\n",
    "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
    "        \n",
    "    parameters:\n",
    "        The phrase is set using the variable seed_phrase\n",
    "        The optional input \"N\" is used to set the number of characters of text to predict.     \n",
    "    '''\n",
    "    x_sequence = [token_to_id[token] for token in seed_phrase]\n",
    "    s.run(tf.assign(h_t,h_t.initial_value))\n",
    "    \n",
    "    #feed the seed phrase, if any\n",
    "    for ix in x_sequence[:-1]:\n",
    "         s.run(tf.assign(h_t,next_h),{x_t:[ix]})\n",
    "    \n",
    "    #start generating\n",
    "    for _ in range(max_length-len(seed_phrase)):\n",
    "        x_probs,_ = s.run([next_probs,tf.assign(h_t,next_h)],{x_t:[x_sequence[-1]]})\n",
    "        x_sequence.append(np.random.choice(n_tokens,p=x_probs[0]))\n",
    "        \n",
    "    return ''.join([tokens[ix] for ix in x_sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    print(generate_sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for _ in range(50):\n",
    "    print(generate_sample(' Trump'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit to coursera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from submit import submit_char_rnn\n",
    "samples = [generate_sample(' Al') for i in range(25)]\n",
    "submission = (history,samples)\n",
    "submit_char_rnn(submission, <email>, <token>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it out!\n",
    "\n",
    "__Disclaimer:__ This assignment is entirely optional. You won't receive bonus points for it. However, it's a fun thing to do. Please share your results on course forums.\n",
    "\n",
    "You've just implemented a recurrent language model that can be tasked with generating any kind of sequence, so there's plenty of data you can try it on:\n",
    "\n",
    "* Novels/poems/songs of your favorite author\n",
    "* News titles/clickbait titles\n",
    "* Source code of Linux or Tensorflow\n",
    "* Molecules in [smiles](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system) format\n",
    "* Melody in notes/chords format\n",
    "* Ikea catalog titles\n",
    "* Pokemon names\n",
    "* Cards from Magic, the Gathering / Hearthstone\n",
    "\n",
    "If you're willing to give it a try, here's what you wanna look at:\n",
    "* Current data format is a sequence of lines, so a novel can be formatted as a list of sentences. Alternatively, you can change data preprocessing altogether.\n",
    "* While some datasets are readily available, others can only be scraped from the web. Try `Selenium` or `Scrapy` for that.\n",
    "* Make sure MAX_LENGTH is adjusted for longer datasets. There's also a bonus section about dynamic RNNs at the bottom.\n",
    "* More complex tasks require larger RNN architecture, try more neurons or several layers. It would also require more training iterations.\n",
    "* Long-term dependencies in music, novels or molecules are better handled with LSTM or GRU\n",
    "\n",
    "__Good hunting!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus level: dynamic RNNs\n",
    "\n",
    "Apart from keras, there's also a friendly tensorflow API for recurrent neural nets. It's based around the symbolic loop function (aka [scan](https://www.tensorflow.org/api_docs/python/tf/scan)).\n",
    "\n",
    "This interface allows for dynamic sequence length and comes with some pre-implemented architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CustomRNN(tf.nn.rnn_cell.BasicRNNCell):\n",
    "    def call(self,input,state):\n",
    "        return rnn_one_step(input[:,0],state)\n",
    "    \n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return n_tokens\n",
    "\n",
    "cell = CustomRNN(rnn_num_units)\n",
    "\n",
    "input_sequence = tf.placeholder('int32',(None,None))\n",
    "    \n",
    "predicted_probas, last_state = tf.nn.dynamic_rnn(cell,input_sequence[:,:,None],\n",
    "                                                 time_major=True,dtype='float32')\n",
    "\n",
    "print predicted_probas.eval({input_sequence:to_matrix(names[:10],max_len=50)}).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we never used MAX_LENGTH in the code above: TF will iterate over however many time-steps you gave it.\n",
    "\n",
    "You can also use the all the pre-implemented RNN cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for obj in dir(tf.nn.rnn_cell)+dir(tf.contrib.rnn):\n",
    "    if obj.endswith('Cell'):\n",
    "        print (obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sequence = tf.placeholder('int32',(None,None))\n",
    "\n",
    "inputs_embedded = embed_x(input_sequence)\n",
    "\n",
    "cell = tf.nn.rnn_cell.LSTMCell(rnn_num_units)\n",
    "\n",
    "state_sequence,last_state = tf.nn.dynamic_rnn(cell,inputs_embedded,dtype='float32')\n",
    "\n",
    "print('LSTM visible states[time,batch,unit]:', state_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
